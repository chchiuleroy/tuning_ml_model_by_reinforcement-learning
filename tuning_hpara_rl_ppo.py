# -*- coding: utf-8 -*-
"""
Created on Thu May 15 17:21:11 2025

@author: roy
"""

'''
Reinforcement learning algorithm:
é›¢æ•£å‹•ä½œç©ºé–“
 Q-Learningï¼ˆé©åˆå°é‡é›¢æ•£è¶…åƒæ•¸ï¼‰
 DQNï¼ˆDeep Q-Networkï¼‰
é€£çºŒå‹•ä½œç©ºé–“
 DDPGï¼ˆDeep Deterministic Policy Gradientï¼‰
 PPOï¼ˆProximal Policy Optimizationï¼‰
 SACï¼ˆSoft Actor-Criticï¼‰
'''

import gym,numpy as np,warnings,pandas as pd,random
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score, KFold
from gym import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback

warnings.filterwarnings('ignore')

data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# è¼‰å…¥è³‡æ–™
X, y = data,target
random_state = random.randint(1, 100000)
kf = KFold(n_splits=10, shuffle=True, random_state=random_state)

# è‡ªè¨‚å¼·åŒ–å­¸ç¿’ç’°å¢ƒ
class RandomForestTuningEnv(gym.Env):
    def __init__(self):
        super().__init__()
        # å‹•ä½œç©ºé–“ï¼šå››å€‹è¶…åƒæ•¸ï¼ˆé€£çºŒè½‰æ•´æ•¸ä½¿ç”¨ï¼‰(è¨­å®šå¥½è¶…åƒæ•¸ç¯„åœ)
        self.action_space = spaces.Box(low=np.array([10,2,2,0.1]), high=np.array([1000,20,20,1.0]), dtype=np.float32) 
        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)  # ç©ºç‹€æ…‹
        self.best_score = -np.inf
        self.last_n = 0
        self.last_d = 0
        self.last_s = 0
        self.last_f = 0
        self.last_reward = 0

    def step(self, action):
        # æ¨¡å‹è¶…åƒæ•¸
        n_estimators = int(action[0])
        max_depth = int(action[1])
        min_samples_split = int(action[2])
        max_features = float(action[3])
        # æ¨¡å‹è¨­å®š
        model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            max_features=max_features,
            n_jobs=-1, # å¤šæ ¸å¿ƒä½¿ç”¨
            random_state=random_state
        )

        # 10-fold CVï¼Œä½¿ç”¨ R square ä½œç‚ºåˆ†æ•¸
        scores = cross_val_score(model, X, y, cv=kf, scoring='r2')
        mean_score = scores.mean()
        
        # normalization å„²å­˜æœ¬è¼ªè³‡è¨Šä½œç‚ºä¸‹æ¬¡è§€å¯Ÿç”¨
        self.last_n = n_estimators / 200
        self.last_d = max_depth / 20
        self.last_s = min_samples_split / 20
        self.last_f = max_features
        self.last_reward = mean_score
        
        obs = np.array([self.last_n,self.last_d,self.last_s,self.last_f,self.last_reward])
        reward = mean_score  # è¶Šé«˜è¶Šå¥½
        done = True
        
        return obs, reward, done, {
            'params': {
                'n_estimators': n_estimators,
                'max_depth': max_depth,
                'min_samples_split': min_samples_split,
                'max_features':max_features,
                'score': mean_score
            }
        }

    def reset(self):
        self.last_n = 0
        self.last_d = 0
        self.last_s = 0
        self.last_f = 0
        self.last_reward = 0
        return np.array([0, 0, 0, 0, 0], dtype=np.float32)

# å»ºç«‹ç’°å¢ƒèˆ‡è¨“ç·´ PPO æ¨¡å‹
env = RandomForestTuningEnv()
model = PPO("MlpPolicy", env, verbose=1)
eval_callback = EvalCallback(
    env,                      # è©•ä¼°æ™‚ä½¿ç”¨çš„ç’°å¢ƒ
    eval_freq=500,            # æ¯ 500 å€‹ timesteps è©•ä¼°ä¸€æ¬¡
    best_model_save_path="./best_model",  # å„²å­˜æœ€ä½³æ¨¡å‹çš„ä½ç½®
    verbose=1
)

model.learn(total_timesteps=20000, callback=eval_callback)

# æ¸¬è©¦ PPO æ¨¡å‹ä¸¦æ‰¾å‡ºæœ€ä½³åƒæ•¸
obs = env.reset()
best_params = None
best_score = -np.inf

for _ in range(10000):
    action, _ = model.predict(obs)
    obs, reward, done, info = env.step(action)
    if reward > best_score:
        best_score = reward
        best_params = info['params']

print("ğŸ“Œ æœ€ä½³è¶…åƒæ•¸:")
print(best_params)

best_model = PPO.load("./best_model/best_model")

import matplotlib.pyplot as plt

data = np.load("./best_model/evaluations.npz")

timesteps = data["timesteps"]
results = data["results"]  # æ¯æ¬¡è©•ä¼°æ™‚çš„ reward (n_eval_episodes å€‹)
mean_rewards = results.mean(axis=1)
std_rewards = results.std(axis=1)

# ç¹ªåœ– learning curve
plt.figure(figsize=(10, 6))
plt.plot(timesteps, mean_rewards, label="Mean reward")
plt.fill_between(timesteps, mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.2)
plt.xlabel("Timesteps")
plt.ylabel("Reward (CV Score)")
plt.title("PPO Tuning Performance over Time")
plt.legend()
plt.grid()
plt.show()